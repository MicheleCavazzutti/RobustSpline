#' Pre-processing raw data for thin-plate spline regression
#'
#' Given the data generated by \link{generate} as an input, pre-processes the 
#' dataset to a form suitable for fitting thin-plate spline regression.
#'
#' @param X Matrix of observed values of \code{X} of size 
#'  \code{n}-times-\code{p}, one row per observation, columns corresponding to the 
#'  positions in the rows of \code{tobs}.
#'  
#' @param tobs Domain locations for the observed points of \code{X}. Matrix
#'  of size \code{p}-times-\code{d}, one row per domain point.
#' 
#' @param m Order of the thin-plate spline, positive integer.
#' 
#' @param int_weights Indicator whether the integrals functions are to be 
#' approximated only as a mean of function values (\code{int_weights=FALSE}), or
#' whether they should be computed as weighted sums of function values 
#' (\code{int_weights=TRUE}). In the latter case, weights are computed as
#' proportional to the respective areas of the Voronoi tesselation of the domain
#' \code{I} (works for dimension \code{d==1} or \code{d==2}). For dimension 
#' \code{d==1}, this is equivalent to the length of intervals associated 
#' to adjacent observation points. For \code{d>2}, no weighting is performed.
#' 
#' @param I.method Applicable only if \code{int_weights==TRUE}. 
#' Input method for the complete domain \code{I}
#' where the Voronoi tesselation for obtaining the weights is evaluated. 
#' Takes a value \code{"box"}
#' for \code{I} the smallest axes-aligned box in the domain, or \code{"chull"}
#' for \code{I} being a convex hull of points in the domain. By default set to 
#' \code{"chull"}. In dimension \code{d=1}, the two methods \code{"box"}
#' and \code{"chull"} are equivalent.
#' 
#' @param I Applicable only if \code{int_weights==TRUE}. A set of points 
#' specifying the complete domain \code{I} of the functional data.
#' In general, can be a \code{q}-times-\code{d} matrix, where \code{q}
#' is the number of points and \code{d} is the dimension. The matrix \code{I}
#' then specifies the point from which to compute the domain \code{I}: 
#' (a) If \code{method.I=="chull"}, the domain is the convex hull of \code{I}; 
#' (b) If \code{method.I=="box"}, the domain is the smallest axes-aligned box
#' that contains \code{I}. If \code{I} is \code{NULL} (by default), then 
#' \code{I} is taken to be the same as \code{x}. If \code{I.method=="box"}, 
#' \code{I} can be specified also by a pair of real values \code{a<b}, 
#' in which case we take \code{I} to be the axis-aligned sqare \code{[a,b]^d}.
#'
#' @return A list of values:
#' \itemize{
#'  \item{"Z"}{ A matrix of size \code{n}-times-\code{p+1} for regression 
#'  fitting. Corresponds to transformed regressors \code{X}.}
#'  \item{"H"}{ A penalty matrix of size \code{p+1}-times-\code{p+1} used for
#'  fitting the regression with thin-plate splines.}
#'  \item{"Q"}{ A matrix containing the null space of the rows of Phi of size
#'  \code{p}-times-\code{p-M}, where \code{M} is the number of monomials
#'  used for the construction of the thin-plate spline. This matrix is used to
#'  pass from parameters gamma to parametrization chi.}
#'  \item{"Omega"}{ A matrix of size \code{p}-times-\code{p} containing the 
#'  \link{eta}-transformed matrix of inter-point distances in \code{tobs}.}
#'  \item{"Phi"}{ A matrix of size \code{p}-times-\code{M} corresponding to the
#'  monomial part of the thin-plate spline.}
#'  \item{"degs"}{ A matrix of size \code{M}-times-\code{d} with degrees of the
#'  monomials in each dimension per row. Used for the construction of 
#'  \code{Phi}.}
#'  \item{"tobs"}{ The same as the input parameter \code{tobs}, for later use.}
#'  \item{"M"}{ Number of all monomials used, is equal to 
#'  \code{choose(m+d-1,d)}.}
#'  \item{"m"}{ Order of the spline, positive integer.}
#'  \item{"p"}{ Number of observed time points, positive integer.}
#'  \item{"d"}{ Dimension of domain, positive integer.}
#'  \item{"n"}{ Sample size, positive integer.}
#' }
#'
#' @examples
#' d = 1       # dimension of domain
#' n = 50      # sample size
#' p = 10      # number of observed points in domain
#' p1 = 101 # size of the complete grid
#' alpha0 = 3  # intercept
#' beta0 = function(t) t^2 # regression coefficient function
#' sd.noiseEps = 0.1 # noise standard deviation
#' tgrid = seq(0,1,length=p1) # full grid of observation points
#' basis_fun = function(t,k) return(sin(2*pi*k*t)+t^k)
#' K = 5 # number of basis functions used in the expansion
#' bfX = list()
#' for(k in 1:K) bfX[[k]] = basis_fun(tgrid,k)
#' bcX = matrix(rnorm(n*K,mean=3,sd=5),ncol=K)  # basis coefficients
#' 
#' # Generate the raw data
#' gen = generate(alpha0, beta0, n, d, p, bfX, bcX, sd.noiseEps)
#' 
#' # Preprocess the data
#' X = gen$X; tobs = gen$tobs
#' m = 2; # degree of the thin-spline used
#' tspr = ts_preprocess(X,tobs,m)
#' 
#' # Thin-plate spline ridge regression
#' ridge(tspr$Z,gen$Y,1e-3,tspr$H)

ts_preprocess = function(X, tobs, m, int_weights=TRUE, 
                         I.method = "chull", I=NULL){
  n = nrow(X)
  p = ncol(X)
  d = ncol(tobs)
  
  if(p!=nrow(tobs)) 
    stop("Number of columns of X must equal number of rows of t")
  
  M = choose(m+d-1,d)
  if(p-M<=0) stop(paste("p must be larger than",M))
  if(2*m<=d) stop(paste("m must be larger than",ceiling(d/2)))
  
  # Establishing the weights for integration in one-dimensional domain
  if(int_weights) 
    integral_weights = vorArea(x = tobs, I.method=I.method, I=I) else
      integral_weights = rep(1/p,p)
  # The domain is taken to be the set [0,1]^d
  
  # if(d==1 & int_weights){
  #   if(any(order(tobs)!=1:p))
  #     stop("For numerical integration, values of tobs must be ordered.")
  #   mids = c(0,(tobs[-1]+tobs[-p])/2,1) # midpoints between adjacent intervals
  #   integral_weights = diff(mids)
  #   if(sum(integral_weights)!=1)
  #     stop("Problem with computing integrating weights.")
  #   # weights given in the one-dimensional integration to the points tobs
  # } else integral_weights = rep(1/p,p)
  # # For multidimensional domain the weights are taken to be equal
  
  # Monomials phi of order <m
  allcom = expand.grid(replicate(d, 0:(m-1), simplify=FALSE))
  degs = as.matrix(allcom[rowSums(allcom)<m,,drop=FALSE])
  M = nrow(degs)
  if(M!=choose(m+d-1,d)) stop("Error in degrees of polynomials")
  
  # Fast matrix of Euclidean distances
  Em = matrix(sqrt(rowSums(
    apply(tobs,2,function(x) outer(x,x,"-")^2))),nrow=nrow(tobs))
  
  # Matrix Omega for the penalty term
  Omega = eta(Em,d,m)
  
  # Matrix Phi, monomials evaluated at tobs
  Phi = apply(degs,1,function(x) apply(t(tobs)^x,2,prod))
  # Phi = matrix(nrow=p,ncol=M)
  # for(i in 1:p) for(j in 1:M) Phi[i,j] = prod(tobs[i,]^degs[j,])
  
  # Null space of the rows of Phi p-(p-M)
  Q = MASS::Null(Phi)
  
  # Test that t(Phi)%*%Q = 0
  if(max(abs(t(Phi)%*%Q))>1e-4) 
    stop("Problem in computing the null space")
  if(ncol(Q)!=p-M) 
    stop("Problem in computing the null space")
  
  A = Omega%*%Q
  B = t(Q)%*%A
  Int_W = diag(integral_weights)
  H = cbind(0,rbind(0,
                    cbind(t(A)%*%Int_W%*%A + B, t(A)%*%Int_W%*%Phi),
                    cbind(t(Phi)%*%Int_W%*%A, t(Phi)%*%Int_W%*%Phi)))
  Z = cbind(1,X%*%Int_W%*%Omega%*%Q,X%*%Int_W%*%Phi)
  
  return(list(Z=Z, H=H, Q=Q, Omega=Omega, Phi=Phi, 
              degs=degs, tobs=tobs, M=M, m=m, p=p, d=d, n=n,
              w=integral_weights))
}

#' Transform the vector of estimated regression coefficients
#'
#' Splits the vector of raw estimated coefficients (output of functions 
#' \link{IRLS}, \link{ridge} or \link{HuberQp}) into parts interpretable in the setup of 
#' thin-plate spline regression setup.
#'
#' @param theta Output vector of raw results of length \code{p+1} from function
#' \link{IRLS}, \link{ridge} or \link{HuberQp}.
#'
#' @param tspr Output of \link{ts_preprocess}.
#'
#' @return A list of estimated parameters:
#' \itemize{
#'  \item{"alpha_hat"}{ Estimate of \code{alpha0}, a numerical value.}
#'  \item{"xi_hat"}{ Estimate of \code{xi}, the part of the parameters that
#'  correspond to matrix \code{Omega} when transformed by \code{Q}. A vector
#'  of length \code{(p-M)}.}
#'  \item{"delta_hat"}{ Estimate of \code{delta}, the part of the parameters that
#'  correspond to matrix \code{Phi}. A vector of length \code{M}, the number of
#'  monomials used for the construction of the thin-plate spline.}
#'  \item{"gamma_hat"}{ Estimate of \code{gamma}, the part of the parameters that
#'  correspond to matrix \code{Omega}. It holds true that 
#'  \code{gamma_hat = Q*xi_hat}. A vector of length \code{p}.}
#'  \item{"beta_hat"}{ Estimate of the regression function \code{beta0} 
#'  evaluated at the \code{p} points from \code{tobs}, where \code{X} was
#'  observed.}
#' }
#'
#' @examples
#' d = 1       # dimension of domain
#' n = 50      # sample size
#' p = 10      # number of observed points in domain
#' p1 = 101 # size of the complete grid
#' alpha0 = 3  # intercept
#' beta0 = function(t) t^2 # regression coefficient function
#' sd.noiseEps = 0.1 # noise standard deviation
#' tgrid = seq(0,1,length=p1) # full grid of observation points
#' basis_fun = function(t,k) return(sin(2*pi*k*t)+t^k)
#' K = 5 # number of basis functions used in the expansion
#' bfX = list()
#' for(k in 1:K) bfX[[k]] = basis_fun(tgrid,k)
#' bcX = matrix(rnorm(n*K,mean=3,sd=5),ncol=K)  # basis coefficients
#' 
#' # Generate the raw data
#' gen = generate(alpha0, beta0, n, d, p, bfX, bcX, sd.noiseEps)
#' 
#' # Preprocess the data
#' X = gen$X; Y = gen$Y; tobs = gen$tobs
#' m = 2; # degree of thin-spline
#' tspr = ts_preprocess(X,tobs,m)
#' 
#' # Thin-plate spline ridge regression
#' res = ridge(tspr$Z,Y,1e-3,tspr$H)
#' 
#' # Transform the coefficients
#' transform_theta(res$theta_hat, tspr)

transform_theta = function(theta,tspr){
  p = tspr$p
  M = tspr$M
  Q = tspr$Q
  Omega = tspr$Omega
  Phi = tspr$Phi
  xihat=theta[2:(p-M+1)]
  dhat=theta[(p-M+2):(p+1)]
  ghat = Q%*%xihat
  return(list(alpha_hat=theta[1],xi_hat=xihat,
              delta_hat=dhat,gamma_hat=ghat,
              beta_hat = c(Omega%*%ghat + Phi%*%dhat)))
}

#' Robust thin-plate splines regression
#'
#' Fits a (potentially robust) thin-plates spline in a scalar-on-function 
#' regression problem with discretely observed predictors. The tuning parameter
#' \code{lambda} is selected using a specified cross-validation criterion.
#'
#' @param X Matrix of observed values of \code{X} of size 
#'  \code{n}-times-\code{p}, one row per observation, columns corresponding to the 
#'  positions in the rows of \code{tobs}.
#'
#' @param Y Vector of responses of length \code{n}.
#'  
#' @param tobs Domain locations for the observed points of \code{X}. Matrix
#'  of size \code{p}-times-\code{d}, one row per domain point.
#' 
#' @param m Order of the thin-plate spline, positive integer.
#'
#' @param type The type of the loss function used in the minimization problem.
#' Accepted are \code{type="absolute"} for the absolute loss \code{rho(t)=|t|/2};
#' \code{type="quantile"} for the (asymmetric) quantile loss 
#' \code{rho(t)=t(alpha-I[t<0])} (\code{absolute} loss with \code{alpha=1/2});
#' \code{type="square"} for the square loss \code{rho(t)=t^2}; 
#' \code{type="Huber"} for the Huber loss \code{rho(t)=t^2/2} if 
#' \code{|t|<tuning} and \code{rho(t)=tuning*(|t|-tuning/2)} otherwise; and 
#' \code{type="logistic"} for the logistic loss 
#' \code{rho(t)=2*t + 4*log(1+exp(-t))-4*log(2)}.
#' 
#' @param alpha The order of the quantile if \code{type="quantile"}. By default
#' taken to be \code{alpha=1/2}, which gives the absolute loss 
#' (\code{type="absolute"}).
#' 
#' @param jcv A numerical indicator of the cross-validation method used to 
#' select the tuning parameter \code{lambda}. The criteria are always 
#' based on the residuals (\code{resids}) and hat values (\code{hats}) in
#' the fitted models. Possible values are:
#' \itemize{
#'  \item{"all"}{ All the criteria below are considered.}
#'  \item{"AIC"}{ Akaike's information criterion given by 
#'  \code{mean(resids^2)+log(n)*mean(hats)}, where \code{n} is the length of
#'  both \code{resids} and \code{hats}.}
#'  \item{"GCV"}{ Leave-one-out cross-validation criterion given by
#'  \code{mean((resids^2)/((1-hats)^2))}.}
#'  \item{"GCV(tr)"}{ Modified leave-one-out cross-validation criterion 
#'  given by \code{mean((resids^2)/((1-mean(hats))^2))}.}
#'  \item{"BIC"}{ Bayes information criterion given by 
#'  \code{mean(resids^2)+2*mean(hats)}.}
#'  \item{"rGCV"}{ A robust version of \code{GCV} where mean is replaced
#'  by a robust M-estimator of scale of \code{resids/(1-hats)}, see 
#'  \link[robustbase]{scaleTau2} for details.}
#'  \item{"rGCV(tr)"}{ Modified version of a \code{rGCV} given by 
#'  a robust M-estimator of scale of \code{resids/(1-mean(hats))}.}
#'  \item{"custom"}{ The custom criterion given by function \code{custfun}. 
#'  Works only if \code{custfun} is part of the input.}
#'  }
#'  
#' @param sc Scale parameter to be used in the IRLS. By default \code{sc=1}, 
#' that is no scaling is performed.
#' 
#' @param vrs Version of the algorhitm to be used in function \link{IRLS}; 
#' either \code{vrs="C"} for the \code{C++} version, or \code{vrs="R"} for the 
#' \code{R} version. Both should give (nearly) identical results, see 
#' \link{IRLS}.
#' 
#' @param plotCV Indicator of whether a plot of the evaluated cross-validation 
#' criteria as a function of \code{lambda} should be given.
#' 
#' @param lambda_grid An optional grid for select \code{lambda} from. By default
#' this is set to be an exponential of a grid of 51 equidistant values
#' in the interval from -28 to -1. 
#'
#' @param custfun A custom function combining the residuals \code{resids} and
#' the hat values \code{hats}. The result of the function must be numeric, see 
#' \link{GCV_crit}.
#' 
#' @param int_weights Indicator whether the integrals functions are to be 
#' approximated only as a mean of function values (\code{int_weights=FALSE}), or
#' whether they should be computed as weighted sums of function values 
#' (\code{int_weights=TRUE}). In the latter case, weights are computed as
#' proportional to the respective areas of the Voronoi tesselation of the domain
#' \code{I} (works for dimension \code{d==1} or \code{d==2}). For dimension 
#' \code{d==1}, this is equivalent to the length of intervals associated 
#' to adjacent observation points. For \code{d>2}, no weighting is performed.
#' 
#' @param I.method Applicable only if \code{int_weights==TRUE}. 
#' Input method for the complete domain \code{I}
#' where the Voronoi tesselation for obtaining the weights is evaluated. 
#' Takes a value \code{"box"}
#' for \code{I} the smallest axes-aligned box in the domain, or \code{"chull"}
#' for \code{I} being a convex hull of points in the domain. By default set to 
#' \code{"chull"}. In dimension \code{d=1}, the two methods \code{"box"}
#' and \code{"chull"} are equivalent.
#' 
#' @param I Applicable only if \code{int_weights==TRUE}. A set of points 
#' specifying the complete domain \code{I} of the functional data.
#' In general, can be a \code{q}-times-\code{d} matrix, where \code{q}
#' is the number of points and \code{d} is the dimension. The matrix \code{I}
#' then specifies the point from which to compute the domain \code{I}: 
#' (a) If \code{method.I=="chull"}, the domain is the convex hull of \code{I}; 
#' (b) If \code{method.I=="box"}, the domain is the smallest axes-aligned box
#' that contains \code{I}. If \code{I} is \code{NULL} (by default), then 
#' \code{I} is taken to be the same as \code{x}. If \code{I.method=="box"}, 
#' \code{I} can be specified also by a pair of real values \code{a<b}, 
#' in which case we take \code{I} to be the axis-aligned sqare \code{[a,b]^d}.
#' 
#' @param resids.in Initialization of the vector of residuals used to launch 
#' the IRLS algorithms. Optional.
#' 
#' @param toler A small positive constant specifying the tolerance level for 
#' terminating the algorithm. The prcedure stops if the maximum absolute 
#' distance between the residuals in the previous iteration and the new 
#' residuals drops below \code{toler}.
#' 
#' @param imax Maximum number of allowed iterations of IRLS. 
#'
#' @param tolerGCV A small positive constant specifying the tolerance level for 
#' terminating the algorithm when the cross-validation is performed. 
#' The prcedure stops if the maximum absolute 
#' distance between the residuals in the previous iteration and the new 
#' residuals drops below \code{tolerGCV}.
#' 
#' @param imaxGCV Maximum number of allowed iterations of IRLS in 
#' cross-validation procedures.
#' 
#' @param echo An indicator whether diagnostic messages should be printed. Use
#' only when modifying the content of the function.
#' 
#' @return The output differs depending whether \code{jcv="all"} or 
#' not. If a specific cross-validation method is selected (that is, 
#' \code{jcv} is not \code{"all"}), a list is returned:
#'  \itemize{
#'  \item{"lambda"}{ The selected tuning parameter \code{lambda} that minimizes
#'  the chosen cross-validation criterion.}
#'  \item{"fitted"}{ A vector of \code{n} fitted values using the tuning 
#'  parameter \code{lambda}.}
#'  \item{"theta_hat"}{ A numerical matrix of size \code{p+1}-times-\code{1} of 
#'  estimated regression coefficients from \link{IRLS}.}
#'  \item{"beta_hat"}{ Estimate of the regression function \code{beta0} 
#'  evaluated at the \code{p} points from \code{tobs}, where \code{X} was
#'  observed.}
#'  \item{"alpha_hat"}{ Estimate of \code{alpha0}, a numerical value.}
#'  \item{"hat_values"}{ Diagonal terms of the (possibly penalized) hat 
#'  matrix of the form \code{Z*solve(t(Z)*W*Z+n*lambda*H)*t(Z)*W}, 
#'  where \code{W} is the diagonal weight matrix in the final iteration 
#'  of \link{IRLS}.}
#'  \item{"weights"}{ The vector of weights given to the observations in the 
#'  final iteration of \link{IRLS}. For squared loss (\code{type="square"})
#'  this gives a vector whose all elements are 2.}
#'  \item{"converged"}{ Indicator whether the \link{IRLS} procedure succefully 
#'  converged. Takes value 1 if IRLS converged, 0 otherwise.}
#' }
#' In case when \code{jcv="all"}, all these values are given for each 
#' cross-validation method considered. For \code{lambda}, \code{alpha_hat},
#' and \code{converged} provides a list of length 6 or 7 (depending on 
#' whether \code{custfun} is specified); for \code{fitted}, \code{beta_hat},
#' \code{hat_values}, and \code{weights} it gives a matrix with 6 or 7 
#' columns, each corresponding to one cross-validation method. 
#'
#' @references
#' Ioannis Kalogridis and Stanislav Nagy. (2025). Robust functional regression 
#' with discretely sampled predictors. 
#' \emph{Computational Statistics and Data Analysis}, to appear.
#'
#' @seealso \link{ts_ridge} for a faster (non-robust) version of
#' this method applied with \code{type="square"}.
#' @seealso \link{ts_HuberQp} for a faster robust version of
#' this method applied with \code{type="Huber"}.
#'
#' @examples
#' n = 50      # sample size
#' p = 10      # dimension of predictors
#' X = matrix(rnorm(n*p),ncol=p) # design matrix
#' Y = X[,1]   # response vector
#' tobs = matrix(sort(runif(p)),ncol=1)
#' type = "absolute" # absolute loss
#' 
#' res = ts_reg(X, Y, tobs, m = 2, type = type, jcv = "all", plotCV = TRUE)

ts_reg = function(X, Y, tobs, m, type, alpha=1/2, jcv = "all", 
                  sc = 1, vrs="C", 
                  plotCV=FALSE, lambda_grid=NULL,
                  custfun=NULL, int_weights=TRUE, 
                  I.method = "chull", I=NULL, 
                  resids.in = rep(1,length(Y)),
                  toler=1e-7, imax=1000,
                  tolerGCV=toler, imaxGCV=imax,
                  echo = FALSE){
  
  jcv = match.arg(jcv,c("all", "AIC", "GCV", "GCV(tr)", "BIC", "rGCV", 
                        "rGCV(tr)", "custom"))
  if(jcv=="all") jcv = 0
  if(jcv=="AIC") jcv = 1
  if(jcv=="GCV") jcv = 2
  if(jcv=="GCV(tr)") jcv = 3
  if(jcv=="BIC") jcv = 4
  if(jcv=="rGCV") jcv = 5
  if(jcv=="rGCV(tr)") jcv = 6
  if(jcv=="custom") jcv = 7
  if(jcv==7 & is.null(custfun)) stop("With custom cross-validation, 
                                    cusfun must be provided.")
  
  # pre-processing for thin-plate splines
  tspr = ts_preprocess(X,tobs,m, int_weights=int_weights, 
                       I.method = I.method, I = I)
  # attach(tspr)
  Z = tspr$Z; H = tspr$H; Q = tspr$Q; Omega = tspr$Omega; Phi = tspr$Phi;
  degs = tspr$degs; M = tspr$M; m = tspr$m; p = tspr$p; d = tspr$d; n = tspr$n;
  
  if(is.null(lambda_grid)){
    # define grid for search for lambda
    rho1 = -28  # search range minimium exp(rho1)
    rho2 = -1   # search range maximum exp(rho2)
    lambda_length = 51
    lambda_grid = exp(c(-Inf,seq(rho1,rho2,length=lambda_length-1)))
  } else {
    if(!is.numeric(lambda_grid)) 
      stop("Grid for lambda values must contain numeric values.")
    if(any(lambda_grid<0)) 
      stop("Grid for lambda values must contain non-negative 
           values.")
    lambda_length = length(lambda_grid)
  }
  GCVfull <- Vectorize(
    function(x) GCV(x,
                    Z = Z, Y = Y, H = H, type=type, alpha=alpha,
                    sc = sc, 
                    vrs=vrs,
                    custfun = custfun, 
                    resids.in = resids.in,
                    toler=tolerGCV, imax=imaxGCV))(lambda_grid)
  ncv = nrow(GCVfull)-2
  GCVconverged = GCVfull[ncv+1,]
  GCVic = GCVfull[ncv+2,]
  
  if(echo) print(paste(c("Numbers of iterations in IRLS", 
                         GCVic), collapse=", "))
  
  GCVfull = GCVfull[1:ncv,]
  cvnames = c("AIC","GCV","GCV(tr)","BIC","rGCV","rGCV(tr)",
              "custom")
  if(jcv==0) rownames(GCVfull) = cvnames[1:ncv] # if all the criteria are used 
  
  if(plotCV){
    if(jcv == 0){
      par(mfrow=c(3,2))
      for(i in 1:ncv){
        plot(log(GCVfull[i,])~log(lambda_grid),type="l",
             lwd=2,
             xlab=expression(log(lambda)),
             ylab="CV criterion",
             main = rownames(GCVfull)[i])
        points(log(GCVfull[i,])~log(lambda_grid),
               cex = 1-GCVconverged, col="red", pch=16)
        # abline(h=log(GCVfull[i,1]),lty=2)
        abline(v=log(lambda_grid[which.min(GCVfull[i,])]),lty=2)
      }
      par(mfrow=c(1,1))  
    } else {
      plot(log(GCVfull[jcv,])~log(lambda_grid),type="l",
           lwd=2, xlab=expression(log(lambda)),
           ylab="CV criterion",
           main = cvnames[jcv])
      points(log(GCVfull[jcv,])~log(lambda_grid),
             cex = 1-GCVconverged, col="red", pch=16)
      # abline(h=log(GCVfull[jcv,1]),lty=2)
      abline(v=log(lambda_grid[which.min(GCVfull[jcv,])]),lty=2)
    }
  }
  
  lopt = lambda_grid[apply(GCVfull,1,which.min)]
  
  if(jcv>0){
    lambda = lopt[jcv] # lambda parameter selected
    #
    res = IRLS(Z,Y,lambda,H,type,alpha=alpha,vrs=vrs,sc=sc, 
               resids.in = resids.in, 
               toler=toler, imax=imax)
    res_ts = transform_theta(res$theta_hat,tspr)
    return(list(lambda = lambda,
                fitted = res$fitted, 
                theta_hat = res$theta_hat,
                beta_hat = res_ts$beta_hat, 
                alpha_hat = (res$theta_hat)[1],
                hat_values = res$hat_values,
                weights = res$weights, 
                converged = res$converged))
  } else {
    n = length(Y)
    fitted = matrix(nrow=n,ncol=ncv)
    betahat = matrix(nrow=nrow(tobs), ncol=ncv)
    thetahat = matrix(nrow=nrow(tobs)+1, ncol=ncv)
    alphahat = rep(NA,ncv)
    hatvals = matrix(nrow=n, ncol=ncv)
    weights = matrix(nrow=n, ncol=ncv)
    converged = rep(NA,ncv)
    for(jcv in 1:ncv){
      lambda = lopt[jcv] # lambda parameter selected
      #
      res = IRLS(Z,Y,lambda,H,type,alpha=alpha,vrs=vrs,sc=sc, 
                 resids.in = resids.in,
                 toler=toler, imax=imax)
      res_ts = transform_theta(res$theta_hat,tspr)
      fitted[,jcv] = res$fitted
      thetahat[,jcv] = res$theta_hat
      betahat[,jcv] = res_ts$beta_hat
      alphahat[jcv] = (res$theta_hat)[1]
      hatvals[,jcv] = res$hat_values
      weights[,jcv] = res$weights
      converged[jcv] = res$converged
    }
    return(list(lambda = lopt,
                fitted = fitted, 
                theta_hat = thetahat,
                beta_hat = betahat,
                alpha_hat = alphahat,
                hat_values = hatvals,
                weights = weights, 
                converged = converged))
  }
}

#' (Non-robust) Thin-plate splines regression
#'
#' Fits a (non-robust) thin-plates spline in a scalar-on-function 
#' regression problem with discretely observed predictors. The tuning parameter
#' \code{lambda} is selected using a specified cross-validation criterion.
#'
#' @param X Matrix of observed values of \code{X} of size 
#'  \code{n}-times-{p}, one row per observation, columns corresponding to the 
#'  positions in the rows of \code{tobs}.
#'
#' @param Y Vector of responses of length \code{n}.
#'  
#' @param tobs Domain locations for the observed points of \code{X}. Matrix
#'  of size \code{p}-times-\code{d}, one row per domain point.
#' 
#' @param m Order of the thin-plate spline, positive integer.
#'
#' @param jcv A numerical indicator of the cross-validation method used to 
#' select the tuning parameter \code{lambda}. The criteria are always 
#' based on the residuals (\code{resids}) and hat values (\code{hats}) in
#' the fitted models. Possible values are:
#' \itemize{
#'  \item{"all"}{ All the criteria below are considered.}
#'  \item{"AIC"}{ Akaike's information criterion given by 
#'  \code{mean(resids^2)+log(n)*mean(hats)}, where \code{n} is the length of
#'  both \code{resids} and \code{hats}.}
#'  \item{"GCV"}{ Leave-one-out cross-validation criterion given by
#'  \code{mean((resids^2)/((1-hats)^2))}.}
#'  \item{"GCV(tr)"}{ Modified leave-one-out cross-validation criterion 
#'  given by \code{mean((resids^2)/((1-mean(hats))^2))}.}
#'  \item{"BIC"}{ Bayes information criterion given by 
#'  \code{mean(resids^2)+2*mean(hats)}.}
#'  \item{"rGCV"}{ A robust version of \code{GCV} where mean is replaced
#'  by a robust M-estimator of scale of \code{resids/(1-hats)}, see 
#'  \link[robustbase]{scaleTau2} for details.}
#'  \item{"rGCV(tr)"}{ Modified version of a \code{rGCV} given by 
#'  a robust M-estimator of scale of \code{resids/(1-mean(hats))}.}
#'  \item{"custom"}{ The custom criterion given by function \code{custfun}. 
#'  Works only if \code{custfun} is part of the input.}
#'  }
#'  
#' @param vrs Version of the algorhitm to be used in function \link{ridge}; 
#' either \code{vrs="C"} for the \code{C++} version, or \code{vrs="R"} for the 
#' \code{R} version. Both should give (nearly) identical results, see 
#' \link{ridge}.
#' 
#' @param plotCV Indicator of whether a plot of the evaluated cross-validation 
#' criteria as a function of \code{lambda} should be given.
#' 
#' @param lambda_grid An optional grid for select \code{lambda} from. By default
#' this is set to be an exponential of a grid of 51 equidistant values
#' in the interval from -28 to -1. 
#'
#' @param custfun A custom function combining the residuals \code{resids} and
#' the hat values \code{hats}. The result of the function must be numeric, see 
#' \link{GCV_crit}.
#' 
#' @param int_weights Indicator whether the integrals functions are to be 
#' approximated only as a mean of function values (\code{int_weights=FALSE}), or
#' whether they should be computed as weighted sums of function values 
#' (\code{int_weights=TRUE}). In the latter case, weights are computed as
#' proportional to the respective areas of the Voronoi tesselation of the domain
#' \code{I} (works for dimension \code{d==1} or \code{d==2}). For dimension 
#' \code{d==1}, this is equivalent to the length of intervals associated 
#' to adjacent observation points. For \code{d>2}, no weighting is performed.
#' 
#' @param I.method Applicable only if \code{int_weights==TRUE}. 
#' Input method for the complete domain \code{I}
#' where the Voronoi tesselation for obtaining the weights is evaluated. 
#' Takes a value \code{"box"}
#' for \code{I} the smallest axes-aligned box in the domain, or \code{"chull"}
#' for \code{I} being a convex hull of points in the domain. By default set to 
#' \code{"chull"}. In dimension \code{d=1}, the two methods \code{"box"}
#' and \code{"chull"} are equivalent.
#' 
#' @param I Applicable only if \code{int_weights==TRUE}. A set of points 
#' specifying the complete domain \code{I} of the functional data.
#' In general, can be a \code{q}-times-\code{d} matrix, where \code{q}
#' is the number of points and \code{d} is the dimension. The matrix \code{I}
#' then specifies the point from which to compute the domain \code{I}: 
#' (a) If \code{method.I=="chull"}, the domain is the convex hull of \code{I}; 
#' (b) If \code{method.I=="box"}, the domain is the smallest axes-aligned box
#' that contains \code{I}. If \code{I} is \code{NULL} (by default), then 
#' \code{I} is taken to be the same as \code{x}. If \code{I.method=="box"}, 
#' \code{I} can be specified also by a pair of real values \code{a<b}, 
#' in which case we take \code{I} to be the axis-aligned sqare \code{[a,b]^d}.
#' 
#' @details Function gives a faster (non-iterative) version of the solution
#' of \link{ts_reg} when \code{type="square"} is used. This corresponds to 
#' the ridge version of an estimator.
#'
#' @return The output differs depending whether \code{jcv="all"} or 
#' not. If a specific cross-validation method is selected (that is, 
#' \code{jcv} is not \code{"all"}), a list is returned:
#'  \itemize{
#'  \item{"lambda"}{ The selected tuning parameter \code{lambda} that minimizes
#'  the chosen cross-validation criterion.}
#'  \item{"fitted"}{ A vector of \code{n} fitted values using the tuning 
#'  parameter \code{lambda}.}
#'  \item{"theta_hat"}{ A numerical matrix of size \code{p+1}-times-\code{1} of 
#'  estimated regression coefficients from \link{ridge}.}
#'  \item{"beta_hat"}{ Estimate of the regression function \code{beta0} 
#'  evaluated at the \code{p} points from \code{tobs}, where \code{X} was
#'  observed.}
#'  \item{"alpha_hat"}{ Estimate of \code{alpha0}, a numerical value.}
#'  \item{"hat_values"}{ Diagonal terms of the (possibly penalized) hat 
#'  matrix of the form \code{Z*solve(t(Z)*W*Z+n*lambda*H)*t(Z)*W}, 
#'  where \code{W} is the diagonal weight matrix in the final iteration 
#'  of \link{IRLS}.}
#' }
#' In case when \code{jcv="all"}, all these values are given for each 
#' cross-validation method considered. For \code{lambda} and \code{alpha_hat},
#' provides a list of length 6 or 7 (depending on 
#' whether \code{custfun} is specified); for \code{fitted}, \code{beta_hat},
#' and \code{hat_values} it gives a matrix with 6 or 7 
#' columns, each corresponding to one cross-validation method. 
#'
#' @seealso \link{ts_reg} for a robust version of this method.
#'
#' @references
#' Ioannis Kalogridis and Stanislav Nagy. (2023). Robust functional regression 
#' with discretely sampled predictors. 
#' \emph{Under review}.
#'
#' @examples
#' n = 50      # sample size
#' p = 10      # dimension of predictors
#' X = matrix(rnorm(n*p),ncol=p) # design matrix
#' Y = X[,1]   # response vector
#' tobs = matrix(sort(runif(p)),ncol=1)
#' 
#' res = ts_ridge(X, Y, tobs, m = 2, jcv = "all", plotCV = TRUE)

ts_ridge = function(X, Y, tobs, m, jcv = "all", vrs="C", 
                    plotCV=FALSE, lambda_grid=NULL,
                    custfun=NULL, int_weights=TRUE, 
                    I.method = "chull", I=NULL){
  jcv = match.arg(jcv,c("all", "AIC", "GCV", "GCV(tr)", "BIC", "rGCV", 
                        "rGCV(tr)", "custom"))
  if(jcv=="all") jcv = 0
  if(jcv=="AIC") jcv = 1
  if(jcv=="GCV") jcv = 2
  if(jcv=="GCV(tr)") jcv = 3
  if(jcv=="BIC") jcv = 4
  if(jcv=="rGCV") jcv = 5
  if(jcv=="rGCV(tr)") jcv = 6
  if(jcv=="custom") jcv = 7
  if(jcv==7 & is.null(custfun)) stop("With custom cross-validation, 
                                    cusfun must be provided.")
  
  # pre-processing for thin-plate splines
  tspr = ts_preprocess(X,tobs,m, int_weights=int_weights, 
                       I.method = I.method, I = I)
  # attach(tspr)
  Z = tspr$Z; H = tspr$H; Q = tspr$Q; Omega = tspr$Omega; Phi = tspr$Phi;
  degs = tspr$degs; M = tspr$M; m = tspr$m; p = tspr$p; d = tspr$d; n = tspr$n;
  
  if(is.null(lambda_grid)){
    # define grid for search for lambda
    rho1 = -28  # search range minimium exp(rho1)
    rho2 = -1   # search range maximum exp(rho2)
    lambda_length = 51
    lambda_grid = exp(c(-Inf,seq(rho1,rho2,length=lambda_length-1)))
  } else {
    if(!is.numeric(lambda_grid)) 
      stop("Grid for lambda values must contain numeric values.")
    if(any(lambda_grid<0)) 
      stop("Grid for lambda values must contain non-negative 
           values.")
    lambda_length = length(lambda_grid)
  }
  GCVfull <- Vectorize(
    function(x) GCV_ridge(x,
                          Z = Z, Y = Y, H = H, vrs=vrs,
                          custfun = custfun))(lambda_grid)
  ncv = nrow(GCVfull)
  cvnames = c("AIC","GCV","GCV(tr)","BIC","rGCV","rGCV(tr)",
              "custom")
  if(jcv==0) rownames(GCVfull) = cvnames[1:ncv]  
  
  if(plotCV){
    if(jcv == 0){
      par(mfrow=c(3,2))
      for(i in 1:ncv){
        plot(log(GCVfull[i,])~log(lambda_grid),type="l",
             lwd=2,
             xlab=expression(log(lambda)),
             ylab="CV criterion",
             main=rownames(GCVfull)[i])
        # abline(h=log(GCVfull[i,1]),lty=2)
        abline(v=log(lambda_grid[which.min(GCVfull[i,])]),lty=2)
      }
      par(mfrow=c(1,1))  
    } else {
      plot(log(GCVfull[jcv,])~log(lambda_grid),type="l",
           lwd=2, xlab=expression(log(lambda)),
           ylab="CV criterion",
           main = cvnames[jcv])
      # abline(h=log(GCVfull[jcv,1]),lty=2)
      abline(v=log(lambda_grid[which.min(GCVfull[jcv,])]),lty=2)
    }
  }
  lopt = lambda_grid[apply(GCVfull,1,which.min)]
  
  if(jcv>0){
    lambda = lopt[jcv] # lambda parameter selected
    #
    res = ridge(Z,Y,lambda,H,vrs=vrs)
    res_ts = transform_theta(res$theta_hat,tspr)
    return(list(lambda = lambda,
                fitted = res$fitted, 
                theta_hat = res$theta_hat,
                beta_hat = res_ts$beta_hat, 
                alpha_hat = (res$theta_hat)[1],
                hat_values = res$hat_values,
                resids = res$resids))
  } else {
    n = length(Y)
    fitted = matrix(nrow=n,ncol=ncv)
    thetahat = matrix(nrow=nrow(tobs)+1, ncol=ncv)
    betahat = matrix(nrow=nrow(tobs), ncol=ncv)
    alphahat = rep(NA,ncv)
    hatvals = matrix(nrow=n, ncol=ncv)
    resids = matrix(nrow=n, ncol=ncv)
    for(jcv in 1:ncv){
      lambda = lopt[jcv] # lambda parameter selected
      #
      res = ridge(Z,Y,lambda,H,vrs=vrs)
      res_ts = transform_theta(res$theta_hat,tspr)
      fitted[,jcv] = res$fitted
      thetahat[,jcv] = res$theta_hat
      betahat[,jcv] = res_ts$beta_hat
      alphahat[jcv] = (res$theta_hat)[1]
      hatvals[,jcv] = res$hat_values
      resids[,jcv] = res$resids
    }
    return(list(lambda = lopt,
                fitted = fitted,
                theta_hat = thetahat,
                beta_hat = betahat,
                alpha_hat = alphahat,
                hat_values = hatvals,
                resids = resids))
  }
}

#' Huber Thin-plate splines regression
#'
#' Fits a robust thin-plates spline in a scalar-on-function 
#' regression problem with discretely observed predictors. The tuning parameter
#' \code{lambda} is selected using a specified cross-validation criterion.
#'
#' @param X Matrix of observed values of \code{X} of size 
#'  \code{n}-times-{p}, one row per observation, columns corresponding to the 
#'  positions in the rows of \code{tobs}.
#'
#' @param Y Vector of responses of length \code{n}.
#'  
#' @param tobs Domain locations for the observed points of \code{X}. Matrix
#'  of size \code{p}-times-\code{d}, one row per domain point.
#' 
#' @param m Order of the thin-plate spline, positive integer.
#'
#' @param jcv A numerical indicator of the cross-validation method used to 
#' select the tuning parameter \code{lambda}. The criteria are always 
#' based on the residuals (\code{resids}) and hat values (\code{hats}) in
#' the fitted models. Possible values are:
#' \itemize{
#'  \item{"all"}{ All the criteria below are considered.}
#'  \item{"AIC"}{ Akaike's information criterion given by 
#'  \code{mean(resids^2)+log(n)*mean(hats)}, where \code{n} is the length of
#'  both \code{resids} and \code{hats}.}
#'  \item{"GCV"}{ Leave-one-out cross-validation criterion given by
#'  \code{mean((resids^2)/((1-hats)^2))}.}
#'  \item{"GCV(tr)"}{ Modified leave-one-out cross-validation criterion 
#'  given by \code{mean((resids^2)/((1-mean(hats))^2))}.}
#'  \item{"BIC"}{ Bayes information criterion given by 
#'  \code{mean(resids^2)+2*mean(hats)}.}
#'  \item{"rGCV"}{ A robust version of \code{GCV} where mean is replaced
#'  by a robust M-estimator of scale of \code{resids/(1-hats)}, see 
#'  \link[robustbase]{scaleTau2} for details.}
#'  \item{"rGCV(tr)"}{ Modified version of a \code{rGCV} given by 
#'  a robust M-estimator of scale of \code{resids/(1-mean(hats))}.}
#'  \item{"custom"}{ The custom criterion given by function \code{custfun}. 
#'  Works only if \code{custfun} is part of the input.}
#'  }
#'  
#' @param vrs Version of the algorhitm to be used in function \link{HuberQp}; 
#' either \code{vrs="C"} for the \code{C++} version, or \code{vrs="R"} for the 
#' \code{R} version. Both should give identical results, but the C++ version is (slightly) faster when finer discretizations are used.
#' See \link{HuberQp} for further details.
#' 
#' @param plotCV Indicator of whether a plot of the evaluated cross-validation 
#' criteria as a function of \code{lambda} should be given.
#' 
#' @param lambda_grid An optional grid for select \code{lambda} from. By default
#' this is set to be an exponential of a grid of 51 equidistant values
#' in the interval from -28 to -1. 
#'
#' @param custfun A custom function combining the residuals \code{resids} and
#' the hat values \code{hats}. The result of the function must be numeric, see 
#' \link{GCV_crit}.
#' 
#' @param int_weights Indicator whether the integrals functions are to be 
#' approximated only as a mean of function values (\code{int_weights=FALSE}), or
#' whether they should be computed as weighted sums of function values 
#' (\code{int_weights=TRUE}). In the latter case, weights are computed as
#' proportional to the respective areas of the Voronoi tesselation of the domain
#' \code{I} (works for dimension \code{d==1} or \code{d==2}). For dimension 
#' \code{d==1}, this is equivalent to the length of intervals associated 
#' to adjacent observation points. For \code{d>2}, no weighting is performed.
#' 
#' @param I.method Applicable only if \code{int_weights==TRUE}. 
#' Input method for the complete domain \code{I}
#' where the Voronoi tesselation for obtaining the weights is evaluated. 
#' Takes a value \code{"box"}
#' for \code{I} the smallest axes-aligned box in the domain, or \code{"chull"}
#' for \code{I} being a convex hull of points in the domain. By default set to 
#' \code{"chull"}. In dimension \code{d=1}, the two methods \code{"box"}
#' and \code{"chull"} are equivalent.
#' 
#' @param I Applicable only if \code{int_weights==TRUE}. A set of points 
#' specifying the complete domain \code{I} of the functional data.
#' In general, can be a \code{q}-times-\code{d} matrix, where \code{q}
#' is the number of points and \code{d} is the dimension. The matrix \code{I}
#' then specifies the point from which to compute the domain \code{I}: 
#' (a) If \code{method.I=="chull"}, the domain is the convex hull of \code{I}; 
#' (b) If \code{method.I=="box"}, the domain is the smallest axes-aligned box
#' that contains \code{I}. If \code{I} is \code{NULL} (by default), then 
#' \code{I} is taken to be the same as \code{x}. If \code{I.method=="box"}, 
#' \code{I} can be specified also by a pair of real values \code{a<b}, 
#' in which case we take \code{I} to be the axis-aligned sqare \code{[a,b]^d}.
#' 
#' @details Function gives a faster (non-iterative) version of the solution
#' of \link{ts_reg} when \code{type="Huber"} is used. This corresponds to 
#' the Huber version of an estimator.
#'
#' @return The output differs depending whether \code{jcv="all"} or 
#' not. If a specific cross-validation method is selected (that is, 
#' \code{jcv} is not \code{"all"}), a list is returned:
#'  \itemize{
#'  \item{"lambda"}{ The selected tuning parameter \code{lambda} that minimizes
#'  the chosen cross-validation criterion.}
#'  \item{"fitted"}{ A vector of \code{n} fitted values using the tuning 
#'  parameter \code{lambda}.}
#'  \item{"theta_hat"}{ A numerical matrix of size \code{p+1}-times-\code{1} of 
#'  estimated regression coefficients from \link{HuberQp}.}
#'  \item{"beta_hat"}{ Estimate of the regression function \code{beta0} 
#'  evaluated at the \code{p} points from \code{tobs}, where \code{X} was
#'  observed.}
#'  \item{"alpha_hat"}{ Estimate of \code{alpha0}, a numerical value.}
#'  \item{"hat_values"}{ Diagonal terms of the (possibly penalized) hat 
#'  matrix of the form \code{Z*solve(t(Z)*W*Z+n*lambda*H)*t(Z)*W}, 
#'  where \code{W} is the diagonal weight matrix in the final iteration 
#'  of \link{IRLS}.}
#' }
#' In case when \code{jcv="all"}, all these values are given for each 
#' cross-validation method considered. For \code{lambda} and \code{alpha_hat},
#' provides a list of length 6 or 7 (depending on 
#' whether \code{custfun} is specified); for \code{fitted}, \code{beta_hat},
#' and \code{hat_values} it gives a matrix with 6 or 7 
#' columns, each corresponding to one cross-validation method. 
#'
#' @seealso \link{ts_reg} for an iterative version of this method.
#'
#' @references
#' Ioannis Kalogridis and Stanislav Nagy. (2023). Robust functional regression 
#' with discretely sampled predictors. 
#' \emph{Under review}.
#'
#' @examples
#' n = 50      # sample size
#' p = 10      # dimension of predictors
#' X = matrix(rnorm(n*p),ncol=p) # design matrix
#' Y = X[,1]   # response vector
#' tobs = matrix(sort(runif(p)),ncol=1)
#' 
#' res = ts_HuberQp(X, Y, tobs, m = 2, jcv = "all", plotCV = TRUE)

ts_HuberQp = function(X, Y, tobs, m, jcv = "all", vrs="C", 
                    plotCV=FALSE, lambda_grid=NULL,
                    custfun=NULL, int_weights=TRUE, 
                    I.method = "chull", I=NULL){
  jcv = match.arg(jcv,c("all", "AIC", "GCV", "GCV(tr)", "BIC", "rGCV", 
                        "rGCV(tr)", "custom"))
  if(jcv=="all") jcv = 0
  if(jcv=="AIC") jcv = 1
  if(jcv=="GCV") jcv = 2
  if(jcv=="GCV(tr)") jcv = 3
  if(jcv=="BIC") jcv = 4
  if(jcv=="rGCV") jcv = 5
  if(jcv=="rGCV(tr)") jcv = 6
  if(jcv=="custom") jcv = 7
  if(jcv==7 & is.null(custfun)) stop("With custom cross-validation, 
                                    cusfun must be provided.")
  
  # pre-processing for thin-plate splines
  tspr = ts_preprocess(X,tobs,m, int_weights=int_weights, 
                       I.method = I.method, I = I)
  # attach(tspr)
  Z = tspr$Z; H = tspr$H; Q = tspr$Q; Omega = tspr$Omega; Phi = tspr$Phi;
  degs = tspr$degs; M = tspr$M; m = tspr$m; p = tspr$p; d = tspr$d; n = tspr$n;
  
  if(is.null(lambda_grid)){
    # define grid for search for lambda
    rho1 = -28  # search range minimium exp(rho1)
    rho2 = -1   # search range maximum exp(rho2)
    lambda_length = 51
    lambda_grid = exp(c(-Inf,seq(rho1,rho2,length=lambda_length-1)))
  } else {
    if(!is.numeric(lambda_grid)) 
      stop("Grid for lambda values must contain numeric values.")
    if(any(lambda_grid<0)) 
      stop("Grid for lambda values must contain non-negative 
           values.")
    lambda_length = length(lambda_grid)
  }
  GCVfull <- Vectorize(
    function(x) GCV_HuberQp(x,
                          Z = Z, Y = Y, H = H, vrs=vrs,
                          custfun = custfun))(lambda_grid)
  ncv = nrow(GCVfull)
  cvnames = c("AIC","GCV","GCV(tr)","BIC","rGCV","rGCV(tr)",
              "custom")
  if(jcv==0) rownames(GCVfull) = cvnames[1:ncv]  
  
  if(plotCV){
    if(jcv == 0){
      par(mfrow=c(3,2))
      for(i in 1:ncv){
        plot(log(GCVfull[i,])~log(lambda_grid),type="l",
             lwd=2,
             xlab=expression(log(lambda)),
             ylab="CV criterion",
             main=rownames(GCVfull)[i])
        # abline(h=log(GCVfull[i,1]),lty=2)
        abline(v=log(lambda_grid[which.min(GCVfull[i,])]),lty=2)
      }
      par(mfrow=c(1,1))  
    } else {
      plot(log(GCVfull[jcv,])~log(lambda_grid),type="l",
           lwd=2, xlab=expression(log(lambda)),
           ylab="CV criterion",
           main = cvnames[jcv])
      # abline(h=log(GCVfull[jcv,1]),lty=2)
      abline(v=log(lambda_grid[which.min(GCVfull[jcv,])]),lty=2)
    }
  }
  lopt = lambda_grid[apply(GCVfull,1,which.min)]
  
  if(jcv>0){
    lambda = lopt[jcv] # lambda parameter selected
    #
    res = HuberQp(Z,Y,lambda,H,vrs=vrs)
    res_ts = transform_theta(res$theta_hat,tspr)
    return(list(lambda = lambda,
                fitted = res$fitted, 
                theta_hat = res$theta_hat,
                beta_hat = res_ts$beta_hat, 
                alpha_hat = (res$theta_hat)[1],
                hat_values = res$hat_values,
                resids = res$resids))
  } else {
    n = length(Y)
    fitted = matrix(nrow=n,ncol=ncv)
    thetahat = matrix(nrow=nrow(tobs)+1, ncol=ncv)
    betahat = matrix(nrow=nrow(tobs), ncol=ncv)
    alphahat = rep(NA,ncv)
    hatvals = matrix(nrow=n, ncol=ncv)
    resids = matrix(nrow=n, ncol=ncv)
    for(jcv in 1:ncv){
      lambda = lopt[jcv] # lambda parameter selected
      #
      res = HuberQp(Z,Y,lambda,H,vrs=vrs)
      res_ts = transform_theta(res$theta_hat,tspr)
      fitted[,jcv] = res$fitted
      thetahat[,jcv] = res$theta_hat
      betahat[,jcv] = res_ts$beta_hat
      alphahat[jcv] = (res$theta_hat)[1]
      hatvals[,jcv] = res$hat_values
      resids[,jcv] = res$resids
    }
    return(list(lambda = lopt,
                fitted = fitted,
                theta_hat = thetahat,
                beta_hat = betahat,
                alpha_hat = alphahat,
                hat_values = hatvals,
                resids = resids))
  }
}